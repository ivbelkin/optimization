{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_p(x, p):\n",
    "    '''\n",
    "    Prox-function\n",
    "    '''\n",
    "    return 1 / (p + 1) * x**(p + 1)\n",
    "\n",
    "\n",
    "def hessian_vector_product(f, x, v):\n",
    "    '''\n",
    "    Hessian-vector product: D^2(f) @ v\n",
    "    '''\n",
    "    grad_f = torch.autograd.grad([f(x)], [x], create_graph=True)[0]\n",
    "    z = grad_f @ v\n",
    "    z.backward()\n",
    "    return x.grad\n",
    "\n",
    "\n",
    "def BDGM(f, x_tilda_k, delta, L3):\n",
    "    \n",
    "    grad_f_x_tilda_k = torch.autograd.grad([f(x_tilda_k)], [x_tilda_k], create_graph=True)[0]\n",
    "    \n",
    "    z_0 = x_tilda_k\n",
    "    tau = 3 * delta / (8 * (2 + math.sqrt(2)) * torch.norm(grad_f))\n",
    "    \n",
    "    def D2v(z):\n",
    "        return hessian_vector_product(f, x_tilda_k, z - x_tilda_k)\n",
    "    \n",
    "    def rho_k(z):\n",
    "        '''\n",
    "        Scaling function\n",
    "        '''\n",
    "        return .5 * D2v(f, x_tilda_k, z - x_tilda_k) @ (z - x_tilda_k) \\\n",
    "               + L3 * d_p(z - x_tilda_k, 4)\n",
    "    \n",
    "    def beta_rho_k(z_i, z):\n",
    "        '''\n",
    "        Bregman distance\n",
    "        '''\n",
    "        grad_rho_k = torch.autograd.grad([rho_k(z)])\n",
    "        return rho_k(z) - rho_k(z_i) - grad_rho_k @ (z - z_i)\n",
    "    \n",
    "    def g_x_tilda_k_tau(z):\n",
    "        grad_g_p = torch.autograd.grad([f(x_tilda_k + tau * (z - x_tilda_k))], [x], create_graph=True)[0]\n",
    "        grad_g_n = torch.autograd.grad([f(x_tilda_k - tau * (z - x_tilda_k))], [x], create_graph=True)[0]\n",
    "        return 1 / tau**2 * (grad_g_p + grad_g_n - 2 * grad_f_x_tilda_k)\n",
    "    \n",
    "    def g_phi_k_tau(z):\n",
    "        return grad_f_x_tilda_k + D2v(z) + g_x_tilda_k_tau(z) + L3 * ((z - x_tilda_k)**2).sum() * (z - x_tilda_k)\n",
    "    \n",
    "    i = 0\n",
    "    z_i = z_0\n",
    "    while True:\n",
    "        g_phi_k_tau_z_i = g_phi_k_tau(z_i)\n",
    "        grad_f_z_i = torch.autograd.grad([f(z_i)], [z_i], create_graph=True)[0]\n",
    "        if torch.norm(g_phi_k_tau_z_i) < 1 / 6 * torch.norm(grad_f_z_i) - delta:\n",
    "            break\n",
    "        else:\n",
    "            \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.1000], requires_grad=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.Tensor([1, 1])\n",
    "v.requires_grad_()\n",
    "\n",
    "x = torch.Tensor([0.1, 0.1])\n",
    "x.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3 * x[0] ** 2 + 4 * x[0] * x[1] + x[1] **2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.,  6.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D2v(f, x, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.,  6.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.Tensor([1, 1])\n",
    "v.requires_grad_()\n",
    "\n",
    "x = torch.Tensor([0.1, 0.1])\n",
    "x.requires_grad_()\n",
    "\n",
    "f = 3 * x[0] ** 2 + 4 * x[0] * x[1] + x[1] **2\n",
    "\n",
    "grad_f, = torch.autograd.grad([f], [x], create_graph=True)\n",
    "z = grad_f @ v\n",
    "z.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0800, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0000, 0.6000], grad_fn=<AddBackward0>),)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6000, grad_fn=<DotBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6000, grad_fn=<DotBackward>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v @ grad_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-61-8a527372c2e5>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-61-8a527372c2e5>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    cimport scipy.linalg.cython_lapack as lp\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import cython\n",
    "cimport scipy.linalg.cython_lapack as lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy.linalg.lapack' has no attribute 'ssptrd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-222f26e24d53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mssptrd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'scipy.linalg.lapack' has no attribute 'ssptrd'"
     ]
    }
   ],
   "source": [
    "lp.ssptrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'alglib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bb3ebc22e8de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0malglib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'alglib'"
     ]
    }
   ],
   "source": [
    "import alglib as al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ivb/miniconda3/envs/nlp/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xalglib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\n",
    "    [-1, -2, 1, 2],\n",
    "    [-2, 3, 0, -2],\n",
    "    [1, 0, 2, 1],\n",
    "    [2, -2, 1, 4]\n",
    "]\n",
    "n = 4\n",
    "isupper = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, tau, d, e = xalglib.smatrixtd(a, n, isupper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.9866666666666677, 0.9066666666666658, 0.5000000000000002, 0.5],\n",
       " [-2.0, -1.3200000000000007, -1.6666666666666665, -0.5],\n",
       " [1.0, 0.0, 3.333333333333334, -3.0],\n",
       " [2.0, -2.0, 1.0, 4.0]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.5999999999999999, 1.3333333333333333]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.9866666666666677, -1.3200000000000007, 3.333333333333334, 4.0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9066666666666658, -1.6666666666666665, -3.0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.9866666666666666, -1.32, 3.3333333333333335, 4]\n",
      "[0.9066666666666666, -1.6666666666666667, -3]\n"
     ]
    }
   ],
   "source": [
    "print([149 / 75, -33 / 25, 10 / 3, 4])\n",
    "print([68 / 75, -5 / 3, -3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = xalglib.smatrixtdunpackq(a, n, isupper, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.13333333333333303, -0.7333333333333335, -0.6666666666666666, 0.0],\n",
       " [-0.33333333333333365, -0.6666666666666666, 0.6666666666666666, 0.0],\n",
       " [-0.9333333333333332, 0.13333333333333358, -0.33333333333333326, 0.0],\n",
       " [0.0, 0.0, 0.0, 1.0]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.9866666666666677, 0.9066666666666658, 0.5000000000000002, 0.5],\n",
       " [-2.0, -1.3200000000000007, -1.6666666666666665, -0.5],\n",
       " [1.0, 0.0, 3.333333333333334, -3.0],\n",
       " [2.0, -2.0, 1.0, 4.0]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor(a, dtype=torch.float32)\n",
    "I = torch.eye(4)\n",
    "k = torch.tensor(1, dtype=torch.float32)\n",
    "c = torch.ones(4, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., requires_grad=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -2.,  1.,  2.],\n",
       "        [-2.,  3.,  0., -2.],\n",
       "        [ 1.,  0.,  2.,  1.],\n",
       "        [ 2., -2.,  1.,  4.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., requires_grad=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = c.T @ torch.inverse(A + I * k) @ c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3281, grad_fn=<DotBackward>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.8069)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
