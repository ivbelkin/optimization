{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "project2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6jVkXyz64-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM12G3zj66zd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def square_norm(x):\n",
        "    return torch.sum(torch.tensor(x)**2)\n",
        "\n",
        "def hyperfast_second_order_method(f, y_0, K, L3, delta):\n",
        "    '''\n",
        "    Algorithm 2: Hyperfast Second-Order Method\n",
        "    '''\n",
        "    A_0 = 0\n",
        "    x_0 = y_0 \n",
        "    x_tilda_k = x_0\n",
        "    A_k = A_0\n",
        "    x_k = x_0\n",
        "    y_k = y_0\n",
        "    for k in range(K):\n",
        "        y_k_plus_1 = BDGM(f, x_tilda_k, delta, L3)\n",
        "        y_k_plus_1.requires_grad_()\n",
        "        lambda_min = 2 / (3 * L3 * square_norm(y_k_plus_1 - x_tilda_k))\n",
        "        lambda_max = 1 / (L3 * square_norm(y_k_plus_1 - x_tilda_k))\n",
        "        lambda_k_plus_1 = np.random.uniform(lambda_min, lambda_max)\n",
        "        a_k_plus_1 = 1 / 2 * (lambda_k_plus_1 +\n",
        "                              (lambda_k_plus_1 ** 2 +\n",
        "                               4 * lambda_k_plus_1 * A_k) ** 0.5)\n",
        "        A_k_plus_1 = a_k_plus_1 + A_k\n",
        "        x_tilda_k = A_k * y_k / A_k_plus_1 + a_k_plus_1 * x_k / A_k_plus_1\n",
        "        x_k = x_k - a_k_plus_1 * torch.autograd.grad([f(y_k_plus_1)],\n",
        "                                                     [y_k_plus_1],\n",
        "                                                     create_graph=True)[0]\n",
        "        A_k = A_k_plus_1\n",
        "        y_k = y_k_plus_1      \n",
        "    return y_k\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mwqqTD764-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def d_p(x, p):\n",
        "    '''\n",
        "    Prox-function\n",
        "    '''\n",
        "    return 1 / (p + 1) * x**(p + 1)\n",
        "\n",
        "\n",
        "def hessian_vector_product(f, x, v):\n",
        "    '''\n",
        "    Hessian-vector product: D^2(f) @ v\n",
        "    '''\n",
        "    grad_f = torch.autograd.grad([f(x)], [x], create_graph=True)[0]\n",
        "    z = grad_f @ v\n",
        "    z.backward()\n",
        "    return x.grad\n",
        "\n",
        "\n",
        "def BDGM(f, x_tilda_k, delta, L3):\n",
        "    \n",
        "    grad_f_x_tilda_k = torch.autograd.grad([f(x_tilda_k)], [x_tilda_k], create_graph=True)[0]\n",
        "    \n",
        "    z_0 = x_tilda_k\n",
        "    tau = 3 * delta / (8 * (2 + math.sqrt(2)) * torch.norm(grad_f))\n",
        "    \n",
        "    def D2v(z):\n",
        "        return hessian_vector_product(f, x_tilda_k, z - x_tilda_k)\n",
        "    \n",
        "    def rho_k(z):\n",
        "        '''\n",
        "        Scaling function\n",
        "        '''\n",
        "        return .5 * D2v(f, x_tilda_k, z - x_tilda_k) @ (z - x_tilda_k) \\\n",
        "               + L3 * d_p(z - x_tilda_k, 4)\n",
        "    \n",
        "    def beta_rho_k(z_i, z):\n",
        "        '''\n",
        "        Bregman distance\n",
        "        '''\n",
        "        grad_rho_k = torch.autograd.grad([rho_k(z)])\n",
        "        return rho_k(z) - rho_k(z_i) - grad_rho_k @ (z - z_i)\n",
        "    \n",
        "    def g_x_tilda_k_tau(z):\n",
        "        grad_g_p = torch.autograd.grad([f(x_tilda_k + tau * (z - x_tilda_k))], [x], create_graph=True)[0]\n",
        "        grad_g_n = torch.autograd.grad([f(x_tilda_k - tau * (z - x_tilda_k))], [x], create_graph=True)[0]\n",
        "        return 1 / tau**2 * (grad_g_p + grad_g_n - 2 * grad_f_x_tilda_k)\n",
        "    \n",
        "    def g_phi_k_tau(z):\n",
        "        return grad_f_x_tilda_k + D2v(z) + g_x_tilda_k_tau(z) + L3 * ((z - x_tilda_k)**2).sum() * (z - x_tilda_k)\n",
        "    \n",
        "    i = 0\n",
        "    z_i = z_0\n",
        "    while True:\n",
        "        g_phi_k_tau_z_i = g_phi_k_tau(z_i)\n",
        "        grad_f_z_i = torch.autograd.grad([f(z_i)], [z_i], create_graph=True)[0]\n",
        "        if torch.norm(g_phi_k_tau_z_i) < 1 / 6 * torch.norm(grad_f_z_i) - delta:\n",
        "            break\n",
        "        else:\n",
        "            \n",
        "        i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMNvZEI764-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(x):\n",
        "    return 3 * x[0] ** 2 + 4 * x[0] * x[1] + x[1] **2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HEeIK9Y64-j",
        "colab_type": "code",
        "colab": {},
        "outputId": "87ec13f9-aebc-4790-9123-fc655e8a2870"
      },
      "source": [
        "D2v(f, x, v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([10.,  6.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMsROyxp64-s",
        "colab_type": "code",
        "colab": {},
        "outputId": "656a9626-8d64-49f0-f93c-cd0bfd51e753"
      },
      "source": [
        "v = torch.Tensor([1, 1])\n",
        "v.requires_grad_()\n",
        "\n",
        "x = torch.Tensor([0.1, 0.1])\n",
        "x.requires_grad_()\n",
        "\n",
        "f = 3 * x[0] ** 2 + 4 * x[0] * x[1] + x[1] **2\n",
        "\n",
        "grad_f, = torch.autograd.grad([f], [x], create_graph=True)\n",
        "z = grad_f @ v\n",
        "z.backward()\n",
        "x.grad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([10.,  6.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7R80Rlw64-w",
        "colab_type": "code",
        "colab": {},
        "outputId": "487b2cd3-a869-47b2-b436-a03456280283"
      },
      "source": [
        "f"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0800, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVhIf4ge64-1",
        "colab_type": "code",
        "colab": {},
        "outputId": "195313ed-8032-4e17-dcd0-21840530b5d2"
      },
      "source": [
        "grad_f"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1.0000, 0.6000], grad_fn=<AddBackward0>),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IC4OnaA64-7",
        "colab_type": "code",
        "colab": {},
        "outputId": "58c9ae58-f153-4509-c130-53dfdada06cd"
      },
      "source": [
        "v"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1.], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwav_D9W64_A",
        "colab_type": "code",
        "colab": {},
        "outputId": "a4a56dcc-b3ad-4c0e-cf8b-e8fbba5afb02"
      },
      "source": [
        "z"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.6000, grad_fn=<DotBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj_yaRMd64_I",
        "colab_type": "code",
        "colab": {},
        "outputId": "ef77e837-c823-4b36-d97d-712fbef27dc6"
      },
      "source": [
        "v @ grad_f"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.6000, grad_fn=<DotBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    }
  ]
}